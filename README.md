# Evolution Strategies for Function Optimization

**Master in Artificial Intelligence - Evolutionary Computation**  
**Date:** November 2025

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Project Overview](#-project-overview)
- [Project Structure](#-project-structure)
- [Algorithm Details](#-algorithm-details)
- [Experimental Design](#-experimental-design)
- [Results & Visualization](#-results--visualization)
- [Customization Guide](#-customization-guide)
- [Troubleshooting](#-troubleshooting)
- [References](#-references)

---

## ğŸš€ Quick Start

### 1. Installation

**Recommended: Using Virtual Environment**
```bash
# Create a lightweight venv in the project folder
python3 -m venv .venv

# Activate the virtual environment
# macOS/Linux:
source .venv/bin/activate
# Windows:
.venv\Scripts\activate

# Install required packages
pip install --upgrade pip
pip install -r requirements.txt
```

**Alternative: Global Installation (not recommended)**
```bash
pip install numpy matplotlib pandas
```

### 2. Run Experiments
```bash
# Ensure virtual environment is active
python main.py
```

### 3. Check Results

Results will be generated in the `outputs/` directory:
- `results.csv` - Detailed results for each run
- `summary_statistics.csv` - Aggregated statistics
- `convergence_sphere.png` - Convergence plot for Sphere function
- `convergence_rastrigin.png` - Convergence plot for Rastrigin function
- `comparison_boxplots.png` - Comparative analysis plots

**Expected Runtime:** ~5-10 minutes (depending on your CPU)

---

## ğŸ“– Project Overview

This project implements **Evolution Strategies (ES)** for continuous function optimization. We compare **(Î¼,Î»)-ES** and **(Î¼+Î»)-ES** strategies on different benchmark functions with varying dimensions.

### Implemented Algorithms

- **(Î¼,Î»)-ES:** Selection only from offspring
- **(Î¼+Î»)-ES:** Selection from both parents and offspring
- **Self-adaptive mutation** with learning rates Ï„ and Ï„'

### Benchmark Functions

| Function | Type | Difficulty | Global Minimum |
|----------|------|------------|----------------|
| **Sphere** | Unimodal | Easy | f(0,...,0) = 0 |
| **Rastrigin** | Multimodal | Hard | f(0,...,0) = 0 |
| **Rosenbrock** | Valley-shaped | Medium | f(1,...,1) = 0 |
| **Ackley** | Multimodal | Hard | f(0,...,0) = 0 |

---

## ğŸ“ Project Structure
```
evolution_strategies/
â”‚
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ main.py                        # Main execution script
â”‚
â”œâ”€â”€ src/                           # Source code package
â”‚   â”œâ”€â”€ __init__.py               # Package initialization
â”‚   â”œâ”€â”€ es_params.py              # ES parameters dataclass
â”‚   â”œâ”€â”€ test_functions.py         # Benchmark functions
â”‚   â”œâ”€â”€ evolution_strategy.py     # Core ES algorithm
â”‚   â”œâ”€â”€ experiment_runner.py      # Experiment management
â”‚   â””â”€â”€ visualization.py          # Plotting functions
â”‚
â””â”€â”€ outputs/                       # Generated results (auto-created)
    â”œâ”€â”€ results.csv
    â”œâ”€â”€ summary_statistics.csv
    â”œâ”€â”€ convergence_sphere.png
    â”œâ”€â”€ convergence_rastrigin.png
    â””â”€â”€ comparison_boxplots.png
```

### Module Descriptions

| Module | Responsibility |
|--------|----------------|
| `es_params.py` | Defines ES configuration parameters |
| `test_functions.py` | Benchmark optimization functions |
| `evolution_strategy.py` | Core ES algorithm implementation |
| `experiment_runner.py` | Manages multiple independent runs |
| `visualization.py` | Generates plots and visualizations |
| `main.py` | Orchestrates experiments and output |

---

## ğŸ§¬ Algorithm Details

### Initialization

- **Population:** Uniformly random in search space
- **Mutation Strength (Ïƒ):** Initial value = 0.5

### Self-Adaptation Mechanism

The mutation strength adapts automatically during evolution:
```
Ï„ = 1 / âˆš(2n)          where n = dimension
Ï„' = 1 / âˆš(2âˆšn)

Ïƒ_new = Ïƒ Ã— exp(Ï„' Ã— N(0,1) + Ï„ Ã— N(0,1))
x_new = x + Ïƒ_new Ã— N(0,I)
```

### Selection Strategies

| Strategy | Description | Advantages |
|----------|-------------|------------|
| **(Î¼,Î»)-ES** | Select Î¼ best from Î» offspring only | More exploratory, avoids stagnation |
| **(Î¼+Î»)-ES** | Select Î¼ best from Î¼ parents + Î» offspring | More stable, preserves best solutions |

### Reproduction

- Each offspring created by mutating a random parent
- **No recombination** (crossover) is used in this implementation

---

## ğŸ”¬ Experimental Design

### Parameters Tested

| Parameter | Values |
|-----------|--------|
| **Functions** | Sphere, Rastrigin |
| **Dimensions** | 10, 20 |
| **Population Sizes** | Î¼=15-30, Î»=100-200 |
| **Strategies** | comma, plus |
| **Independent Runs** | 30 per configuration |
| **Max Generations** | 500 |
| **Target Fitness** | 1e-6 |

### Performance Metrics

âœ… **Best fitness achieved**  
âœ… **Generations to convergence**  
âœ… **Function evaluations**  
âœ… **Execution time**  
âœ… **Success rate** (% reaching target fitness)

---

## ğŸ“Š Results & Visualization

### Output Files

#### 1. `results.csv`
Detailed results for each individual run.

**Columns:**
- `function` - Benchmark function name
- `dimension` - Problem dimension
- `mu` - Number of parents
- `lambda` - Number of offspring
- `strategy` - Selection strategy (comma/plus)
- `run` - Run number (1-30)
- `best_fitness` - Best fitness achieved
- `generations` - Generations to convergence
- `function_evals` - Total function evaluations
- `time` - Execution time (seconds)
- `converged` - Boolean: reached target fitness

#### 2. `summary_statistics.csv`
Aggregated statistics per configuration (mean, std, min).

#### 3. Convergence Plots
- **convergence_sphere.png** - Shows fitness evolution over generations for Sphere function
- **convergence_rastrigin.png** - Shows fitness evolution for Rastrigin function
- Mean Â± standard deviation across 30 runs
- Log scale on y-axis for better visualization

#### 4. `comparison_boxplots.png`
Four subplots comparing all configurations:
- Best fitness distribution
- Generations to convergence
- Function evaluations
- Success rate

### Statistical Analysis

Each configuration is run **30 times** to ensure statistical significance.

**Metrics Computed:**
- Mean and standard deviation
- Success rate (% reaching target fitness)
- Minimum achieved fitness
- Median performance

**Comparisons:**
- (Î¼,Î») vs (Î¼+Î») strategies
- Different dimensions (10 vs 20)
- Easy (Sphere) vs Hard (Rastrigin) functions
- Impact of population size

---

## ğŸ›ï¸ Customization Guide

### Modify Experiment Configurations

Edit the `experiments` list in `main.py`:
```python
experiments = [
    {'func': 'sphere', 'dim': 10, 'mu': 15, 'lambda': 100, 'strategy': 'comma'},
    {'func': 'ackley', 'dim': 20, 'mu': 25, 'lambda': 150, 'strategy': 'plus'},
    # Add your custom configurations here
]
```

**Available Functions:**
- `'sphere'`
- `'rastrigin'`
- `'rosenbrock'`
- `'ackley'`

**Recommended Î»/Î¼ Ratio:** 5-10

### Change Number of Runs
```python
runner = ExperimentRunner(n_runs=30)  # Change to 10, 20, etc.
```

### Add New Benchmark Functions

Add to `src/test_functions.py`:
```python
@staticmethod
def your_function(x: np.ndarray) -> float:
    """Your custom optimization function"""
    return np.sum(x**4)  # Example

# Also add bounds in get_bounds() method
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### âŒ `ModuleNotFoundError: No module named 'src'`

**Solution:**
```bash
# Make sure you're in the project root directory
cd path/to/evolution_strategies/
python main.py
```

#### âŒ `ModuleNotFoundError: No module named 'numpy'`

**Solution:**
```bash
pip install -r requirements.txt
```

#### â±ï¸ Experiments Running Too Slow

**Solutions:**
- Reduce `n_runs` from 30 to 10-15
- Reduce `max_generations` from 500 to 200-300
- Test on smaller dimensions first

#### ğŸ’¾ Memory Error

**Solutions:**
- Reduce dimension size
- Reduce population size (Î¼ and Î»)
- Close other applications

#### ğŸ“‰ Poor Convergence on Difficult Functions

**Expected Behavior:** Rastrigin and Ackley are intentionally difficult with many local optima.

**Solutions to Improve:**
- Increase `max_generations`
- Increase population size (larger Î»)
- Adjust Î»/Î¼ ratio (try 7-10)
- Try different random seeds

---

## ğŸ“š References

### Evolution Strategies

1. **Rechenberg, I. (1973).** *Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution.* Stuttgart: Frommann-Holzboog.

2. **Schwefel, H.-P. (1995).** *Evolution and Optimum Seeking.* New York: Wiley.

3. **Hansen, N., & Ostermeier, A. (2001).** "Completely derandomized self-adaptation in evolution strategies." *Evolutionary Computation*, 9(2), 159-195.

4. **Beyer, H.-G., & Schwefel, H.-P. (2002).** "Evolution strategies â€“ A comprehensive introduction." *Natural Computing*, 1(1), 3-52.

### Benchmark Functions

5. **Jamil, M., & Yang, X. S. (2013).** "A literature survey of benchmark functions for global optimization problems." *International Journal of Mathematical Modelling and Numerical Optimisation*, 4(2), 150-194.

6. **Surjanovic, S., & Bingham, D.** *Virtual Library of Simulation Experiments: Test Functions and Datasets.* Retrieved from https://www.sfu.ca/~ssurjano/optimization.html

---

## ğŸ‘¥ Contact & Submission

**Master in Artificial Intelligence**  
**Course:** Evolutionary Computation - Practical Work  
**Submission Deadline:** November 10, 2025

For technical questions about the implementation, refer to:
- Inline documentation in source code
- Module docstrings
- This README

---

## ğŸ“ License

This project is developed for educational purposes as part of the Master in AI curriculum.

---

## âœ¨ Features

âœ… **Modular Design** - Clean separation of concerns  
âœ… **Easy to Extend** - Add new functions or strategies easily  
âœ… **Well-Documented** - Comprehensive docstrings and comments  
âœ… **Statistical Rigor** - 30 independent runs per configuration  
âœ… **Professional Visualizations** - Publication-quality plots  
âœ… **Reproducible** - Fixed random seed for consistency

---

**Happy Experimenting! ğŸ§ªğŸ§¬**